{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee9abb-dd21-4690-ab13-cab16e50f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4bd6a-c255-4451-bef0-af16f164b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# from Segment Anything demo\n",
    "url = 'https://segment-anything.com/assets/gallery/GettyImages-1207721867.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# # from local device\n",
    "# img_path = './demo.jpg'\n",
    "# image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "width, height = image.size\n",
    "display(image.resize((width // 3, height // 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc3ac7-7173-4297-9bd1-ecdccc2afb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4ce03-11f2-42de-9d36-5fa2b6d752b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "\n",
    "# parameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if 'cuda' in device else torch.float32\n",
    "model_type = 'vit_h'\n",
    "checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "\n",
    "# SAM initialization\n",
    "model = sam_model_registry[model_type](checkpoint = checkpoint)\n",
    "model.to(device)\n",
    "predictor = SamPredictor(model)\n",
    "mask_generator = SamAutomaticMaskGenerator(model)\n",
    "predictor.set_image(np.array(image)) # load the image to predictor\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe71c8f-72aa-4bfd-ae2b-982b4c5a5299",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = [[1800, 950]] # A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels.\n",
    "input_label = [1]           # A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.\n",
    "input_point = np.array(input_point)\n",
    "input_label = np.array(input_label)\n",
    "masks, scores, logits = predictor.predict(point_coords = input_point, point_labels = input_label)\n",
    "masks = masks[0, ...]\n",
    "\n",
    "display(Image.fromarray(masks).resize(((width // 3, height // 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e73403-27a2-492a-a07f-06e24d9c7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_mode = \"wo_bg\" # Optional['wo_bg', 'w_bg'], where w_bg and wo_bg refer to remain and discard background separately.\n",
    "\n",
    "if crop_mode == \"wo_bg\":\n",
    "    masked_image = image * masks[:,:,np.newaxis] + (1 - masks[:,:,np.newaxis]) * 255\n",
    "    masked_image = np.uint8(masked_image)\n",
    "else:\n",
    "    masked_image = np.array(image)\n",
    "masked_image = Image.fromarray(masked_image)\n",
    "\n",
    "display(masked_image.resize((width // 3, height // 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2321d9-291e-47e6-800b-ff6eab9586d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary(inputs):\n",
    "    \n",
    "    col = inputs.shape[1]\n",
    "    inputs = inputs.reshape(-1)\n",
    "    lens = len(inputs)\n",
    "    start = np.argmax(inputs)\n",
    "    end = lens - 1 - np.argmax(np.flip(inputs))\n",
    "    top = start // col\n",
    "    bottom = end // col\n",
    "    \n",
    "    return top, bottom\n",
    "\n",
    "def seg_to_box(seg_mask, size):\n",
    "    \n",
    "    top, bottom = boundary(seg_mask)\n",
    "    left, right = boundary(seg_mask.T)\n",
    "    left, top, right, bottom = left / size, top / size, right / size, bottom / size # we normalize the size of boundary to 0 ~ 1\n",
    "\n",
    "    return [left, top, right, bottom]\n",
    "\n",
    "size = max(masks.shape[0], masks.shape[1])\n",
    "left, top, right, bottom = seg_to_box(masks, size) # calculating the position of the top-left and bottom-right corners in the image\n",
    "print(left, top, right, bottom)\n",
    "\n",
    "image_crop = masked_image.crop((left * size, top * size, right * size, bottom * size)) # crop the image\n",
    "display(image_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a7252-2a5b-446f-b1db-f0625ab3867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22d534-15f3-4793-8293-1cfd426a0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "captioning_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map = \"sequential\", load_in_8bit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affe909-6ec6-4e51-9048-32f09b09996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(image_crop, return_tensors = \"pt\").to(device, torch_dtype)\n",
    "out = captioning_model.generate(**inputs, max_new_tokens = 50)\n",
    "captions = processor.decode(out[0], skip_special_tokens = True).strip()\n",
    "\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2edc26-54d6-4367-b691-e0bebf93a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = 'Question: What does the image show? Answer:'\n",
    "\n",
    "inputs = processor(image_crop, text = text_prompt, return_tensors = \"pt\").to(device, torch_dtype)\n",
    "out = captioning_model.generate(**inputs, max_new_tokens = 50)\n",
    "captions = processor.decode(out[0], skip_special_tokens = True).strip()\n",
    "\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfdd7a-2039-4da7-928e-3d75efaa6c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fb048-14a1-4a39-94df-12f304ac7f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fc45b-5b0b-431f-9d07-c56954ecfbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
